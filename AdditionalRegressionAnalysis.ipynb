{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Regression Analyses\n",
    "## Summary and Initial Investigation Recap\n",
    "The Initial Regression investigation showed that the best results were found with the non-sampled stemmatized reviews which were vectorized with the Term Frequency-Inverse Document Frequency (TFIDF) method. \n",
    "\n",
    "Regression was measured using the LinearRegression model only and the best prediction of the test sample provided by this data included the following scores:\n",
    "* Coefficient: 0.755\n",
    "* R Squared Value: 0.612\n",
    "* P Value: 0 to at least 5dp\n",
    "* Precision: 0.39\n",
    "* F1 Score: 0.34\n",
    "* Geometric Mean: 0.52\n",
    "\n",
    "These scores are not good enough to accurately provide a sentiment analysis of the reviews, but the P Value is very promising.\n",
    "\n",
    "The initial Analysis also revealed that the text data could be adjusted to fit the overall star rating from 1-5, however the final results must be cut into values of 1 to 5 integers otherwise the errors contained in the float decimal skew the regression error metrics like the geometric mean. It was also observed that the predicted values were not contained within the 1 - 5 bounds and a small amount of values drifted beyond Â±10.\n",
    "\n",
    "The data was unbalanced, and due to the quantity of reviews available only under sampling resampling was attempted on the training data, however the resampled data had a negative impact on all scores.\n",
    "\n",
    "## Hypothesis\n",
    "The Initial Regression Analysis P Value indicates that there is indeed a correlation between the reviews and the review rating scores from 1-5, since the linear regression model did not provide satisfactory scores the correlation may not be linear.\n",
    "\n",
    "The resampling was not sufficient in the Initial exploration, however we can clearly see that the datset is imbalanced with 71.9% of the values falling in 1 and 5. Due to the amount of data only under sampling was implemented, however by reducing the size of the training set other sampling methods could be implemented. Of course different sampling methods may affect the data differently, so it may be necessary to lemmatize and stemmatize the original texts for different methods. Similarly different models may provide better scores with the vectorization methods.\n",
    "\n",
    "By implementing pipelines and GridSearchCV, several regression models can be tested quickly and return the best scores and parameters for each model can be returned. In this notebook the data will be lemmatized, stemmatized, CountVectorized and TFIDF Vectorized as before. In addition the training set will be halved and RandomOverSampling, Synthetic Minority Over-sampling (SMOTE) and CentroidCluster resampling techniques will be implemented alongside RandomUndersampling. The raw data alongside the resampled data will be dimensionally reduced using Lasso, Random Forest, XGBoost and Principle Component Analysis (PCA) Methods. Finally Regression analyses will be performed by fitting and predicting data using Lasso, Ridge, ElasticNet and GradientBoostingRegressor models.\n",
    "\n",
    "It may be that the data is better suited to Classification than Regression, and so whilst this document is prepared to analyse Regression a second notebook is being prepared to model the data with classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             good like \n",
      "1    happy best product \n",
      "2    good happy product \n",
      "3      bad stuff broken \n",
      "4        evil bad thing \n",
      "5          good product \n",
      "dtype: object\n",
      "[[0.         0.         0.         0.         0.56921261 0.\n",
      "  0.82219037 0.         0.         0.        ]\n",
      " [0.         0.68172171 0.         0.         0.         0.55902156\n",
      "  0.         0.47196441 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.54209195 0.64208461\n",
      "  0.         0.54209195 0.         0.        ]\n",
      " [0.50161301 0.         0.61171251 0.         0.         0.\n",
      "  0.         0.         0.61171251 0.        ]\n",
      " [0.50161301 0.         0.         0.61171251 0.         0.\n",
      "  0.         0.         0.         0.61171251]\n",
      " [0.         0.         0.         0.         0.70710678 0.\n",
      "  0.         0.70710678 0.         0.        ]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from TextMiningProcesses import column_lemmatizer, column_stemmatizer, count_vectorize_data, tfidf_vectorize_data\n",
    "\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.feature_selection import RFE, SelectKBest, SelectFromModel\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, LinearRegression\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train - Test Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of Training data records: 43043\n",
      "Original number of Test data records:18447\n",
      "Training data will be shortened to enable resampling within a reasonable timeframe\n",
      "New number of Training data records 21521\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# View length of data\n",
    "print(f'Original number of Training data records: {len(train)}')\n",
    "print(f'Original number of Test data records:{len(test)}')\n",
    "\n",
    "# half the amount of training data for sampling\n",
    "half_train = train.iloc[:len(train) // 2, :]\n",
    "\n",
    "# View length of data\n",
    "print(f'Training data will be shortened to enable resampling within a reasonable timeframe')\n",
    "print(f'New number of Training data records {len(half_train)}')\n",
    "\n",
    "# Create X_train, X_test, y_train, y_test variables\n",
    "X_train = half_train['reviewText']\n",
    "y_train = half_train['overall']\n",
    "X_test = test['reviewText']\n",
    "y_test = test['overall']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cutting and Scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CutTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_cut = pd.cut(x=X, bins=[X.min(),1.5,2.5,3.5,4.5,X.max()], labels=[1,2,3,4,5], include_lowest=True)\n",
    "        return X_cut\n",
    "    \n",
    "\n",
    "precision_scorer = make_scorer(precision_score, average='weighted')\n",
    "recall_scorer = make_scorer(recall_score, average='weighted')\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "    \n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': precision_scorer,\n",
    "    'recall': recall_scorer,\n",
    "    'f1_score': f1_scorer,\n",
    "    'r2_score': 'r2',\n",
    "    'mse': mse_scorer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lemmatizer=True):\n",
    "        self.lemmatizer = lemmatizer\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.lemmatizer:\n",
    "            tokenized_X = column_lemmatizer(X)\n",
    "            print(f'Lemmatization Shape: {tokenized_X.shape}')\n",
    "            return tokenized_X\n",
    "        else:\n",
    "            tokenized_X = column_stemmatizer(X)\n",
    "            print(f'Stemmatization Shape: {tokenized_X.shape}')\n",
    "            return tokenized_X\n",
    "    \n",
    "tokens = Tokenizer(lemmatizer=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Shape: (21521,)\n",
      "bought cooker order able make chinese hot pot home instruction set easy follow even come cooking pot box included magnet make easy test pan pot see work cooker purchasing setting simple easy use cord nice enough length extend outlet kitchen table happy purchase wait find us \n",
      "21521\n",
      "21521\n"
     ]
    }
   ],
   "source": [
    "# Testing text functions\n",
    "\n",
    "tokens = Tokenizer(lemmatizer=True)\n",
    "\n",
    "processed_text = tokens.transform(X_train)\n",
    "\n",
    "print(processed_text[0])\n",
    "print(len(X_train))\n",
    "print(len(processed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, CountVector=True, max_features=None):\n",
    "        self.CountVector = CountVector\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.CountVector:\n",
    "            vectorized_x = count_vectorize_data(X, max_features=self.max_features)\n",
    "            print(f'CVector shape: {vectorized_x[0].shape}')\n",
    "            return vectorized_x[0]\n",
    "        else:\n",
    "            vectorized_x = tfidf_vectorize_data(X, max_features=self.max_features)\n",
    "            print(f'TFIDF shape: {vectorized_x[0].shape}')\n",
    "            return vectorized_x[0]\n",
    "        \n",
    "Vector = Vectorizer(CountVector=True, max_features=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Shape: (21521,)\n",
      "CVector shape: (21521, 8000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>overall</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2227</td>\n",
       "      <td>184</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2037</td>\n",
       "      <td>635</td>\n",
       "      <td>255</td>\n",
       "      <td>62</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1305</td>\n",
       "      <td>577</td>\n",
       "      <td>1004</td>\n",
       "      <td>699</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>112</td>\n",
       "      <td>93</td>\n",
       "      <td>382</td>\n",
       "      <td>1637</td>\n",
       "      <td>4196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>527</td>\n",
       "      <td>4728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "overall     1    2     3     4     5\n",
       "row_0                               \n",
       "1        2227  184    26     2     0\n",
       "2        2037  635   255    62    30\n",
       "3        1305  577  1004   699   768\n",
       "4         112   93   382  1637  4196\n",
       "5           0    3    32   527  4728"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer, vectorizer, cutter test\n",
    "\n",
    "tokens = Tokenizer(lemmatizer=True)\n",
    "\n",
    "Vector = Vectorizer(CountVector=True, max_features=8000)\n",
    "\n",
    "cutter = CutTransformer()\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "X_tokens = tokens.transform(X_train)\n",
    "X_vector = Vector.transform(X_tokens)\n",
    "lr.fit(X_vector, y_train)\n",
    "y_pred = lr.predict(X_vector)\n",
    "y_cut = cutter.transform(y_pred)\n",
    "\n",
    "y_cut.describe()\n",
    "\n",
    "display(pd.crosstab(y_cut, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, clusters=5, state=0, k_neighbors=50):\n",
    "        self.clusters = clusters\n",
    "        self.estimator = KMeans(n_clusters=self.clusters, random_state=69)\n",
    "        self.state = state\n",
    "        self.k_neighbors = k_neighbors\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        if self.state == 0:\n",
    "            oversampler = RandomOverSampler(sampling_strategy='minority', random_state=69)\n",
    "            X_sampled, y_sampled = oversampler.fit_resample(X_train, y_train)\n",
    "            print(f'rOs size: {X_sampled.size, y_sampled.size}')\n",
    "            return X_sampled, y_sampled\n",
    "        \n",
    "        elif self.state == 1:\n",
    "            smotesampler = SMOTE(sampling_strategy='minority', random_state=69, k_neighbors=self.k_neighbors)\n",
    "            X_sampled, y_sampled = smotesampler.fit_resample(X_train, y_train)\n",
    "            print(f'smote size: {X_sampled.size, y_sampled.size}, k_neighbors ={self.k_neighbors}')\n",
    "            return X_sampled, y_sampled\n",
    "        \n",
    "        elif self.state == 2:\n",
    "            undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=69)\n",
    "            X_sampled, y_sampled = undersampler.fit_resample(X_train, y_train)\n",
    "            print(f'rUs size: {X_sampled.size, y_sampled.size}')\n",
    "            return X_sampled, y_sampled\n",
    "        \n",
    "        elif self.state == 3:\n",
    "            ccsampler = ClusterCentroids(sampling_strategy='majority', random_state=69, estimator=self.estimator)\n",
    "            X_sampled, y_sampled = ccsampler.fit_resample(X_train, y_train)\n",
    "            print(f'Cluster size: {X_sampled.size, y_sampled.size}, clusters = {self.clusters}')\n",
    "            return X_sampled, y_sampled\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('State outside 0-3')\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "sample = Sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Shape: (10760,)\n",
      "CVector shape: (10760, 19833)\n",
      "Lemmatization Shape: (10761,)\n",
      "CVector shape: (10761, 18825)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:821: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 810, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 760, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 760, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 386, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 369, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 626, in _validate_data\n",
      "    self._check_n_features(X, reset=reset)\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 415, in _check_n_features\n",
      "    raise ValueError(\n",
      "ValueError: X has 18825 features, but LinearRegression is expecting 19833 features as input.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Shape: (10761,)\n",
      "CVector shape: (10761, 18825)\n",
      "Lemmatization Shape: (10760,)\n",
      "CVector shape: (10760, 19833)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:821: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 810, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 760, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 760, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 386, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 369, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 626, in _validate_data\n",
      "    self._check_n_features(X, reset=reset)\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 415, in _check_n_features\n",
      "    raise ValueError(\n",
      "ValueError: X has 19833 features, but LinearRegression is expecting 18825 features as input.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmatization Shape: (10760,)\n",
      "CVector shape: (10760, 15078)\n",
      "Stemmatization Shape: (10761,)\n",
      "CVector shape: (10761, 14091)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:821: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 810, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 760, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 760, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 386, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 369, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 626, in _validate_data\n",
      "    self._check_n_features(X, reset=reset)\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 415, in _check_n_features\n",
      "    raise ValueError(\n",
      "ValueError: X has 14091 features, but LinearRegression is expecting 15078 features as input.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmatization Shape: (10761,)\n",
      "CVector shape: (10761, 14091)\n",
      "Stemmatization Shape: (10760,)\n",
      "CVector shape: (10760, 15078)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:821: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 810, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 760, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 760, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 386, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 369, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 626, in _validate_data\n",
      "    self._check_n_features(X, reset=reset)\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 415, in _check_n_features\n",
      "    raise ValueError(\n",
      "ValueError: X has 15078 features, but LinearRegression is expecting 14091 features as input.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Shape: (10760,)\n",
      "TFIDF shape: (10760, 19833)\n",
      "Lemmatization Shape: (10761,)\n",
      "TFIDF shape: (10761, 18825)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:821: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 810, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 760, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 760, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 386, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 369, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 626, in _validate_data\n",
      "    self._check_n_features(X, reset=reset)\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 415, in _check_n_features\n",
      "    raise ValueError(\n",
      "ValueError: X has 18825 features, but LinearRegression is expecting 19833 features as input.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Shape: (10761,)\n",
      "TFIDF shape: (10761, 18825)\n",
      "Lemmatization Shape: (10760,)\n",
      "TFIDF shape: (10760, 19833)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:821: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 810, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 760, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 760, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 386, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 369, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 626, in _validate_data\n",
      "    self._check_n_features(X, reset=reset)\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 415, in _check_n_features\n",
      "    raise ValueError(\n",
      "ValueError: X has 19833 features, but LinearRegression is expecting 18825 features as input.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmatization Shape: (10760,)\n",
      "TFIDF shape: (10760, 15078)\n",
      "Stemmatization Shape: (10761,)\n",
      "TFIDF shape: (10761, 14091)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:821: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 810, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 760, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 760, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 386, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 369, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 626, in _validate_data\n",
      "    self._check_n_features(X, reset=reset)\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 415, in _check_n_features\n",
      "    raise ValueError(\n",
      "ValueError: X has 14091 features, but LinearRegression is expecting 15078 features as input.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmatization Shape: (10761,)\n",
      "TFIDF shape: (10761, 14091)\n",
      "Stemmatization Shape: (10760,)\n",
      "TFIDF shape: (10760, 15078)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:821: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 810, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 760, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 760, in score\n",
      "    y_pred = self.predict(X)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 386, in predict\n",
      "    return self._decision_function(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 369, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 626, in _validate_data\n",
      "    self._check_n_features(X, reset=reset)\n",
      "  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 415, in _check_n_features\n",
      "    raise ValueError(\n",
      "ValueError: X has 15078 features, but LinearRegression is expecting 14091 features as input.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Shape: (21521,)\n",
      "CVector shape: (21521, 27015)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_CVector__CountVector</th>\n",
       "      <th>param_Text_PP__lemmatizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.226485</td>\n",
       "      <td>0.119513</td>\n",
       "      <td>1.267871</td>\n",
       "      <td>0.002429</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>{'CVector__CountVector': True, 'Text_PP__lemma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.956761</td>\n",
       "      <td>0.754733</td>\n",
       "      <td>2.929029</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'CVector__CountVector': True, 'Text_PP__lemma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.012377</td>\n",
       "      <td>1.251978</td>\n",
       "      <td>1.261772</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'CVector__CountVector': False, 'Text_PP__lemm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.398927</td>\n",
       "      <td>1.283020</td>\n",
       "      <td>2.921455</td>\n",
       "      <td>0.002443</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'CVector__CountVector': False, 'Text_PP__lemm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       8.226485      0.119513         1.267871        0.002429   \n",
       "1      10.956761      0.754733         2.929029        0.004717   \n",
       "2      19.012377      1.251978         1.261772        0.002349   \n",
       "3       6.398927      1.283020         2.921455        0.002443   \n",
       "\n",
       "  param_CVector__CountVector param_Text_PP__lemmatizer  \\\n",
       "0                       True                      True   \n",
       "1                       True                     False   \n",
       "2                      False                      True   \n",
       "3                      False                     False   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'CVector__CountVector': True, 'Text_PP__lemma...                NaN   \n",
       "1  {'CVector__CountVector': True, 'Text_PP__lemma...                NaN   \n",
       "2  {'CVector__CountVector': False, 'Text_PP__lemm...                NaN   \n",
       "3  {'CVector__CountVector': False, 'Text_PP__lemm...                NaN   \n",
       "\n",
       "   split1_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0                NaN              NaN             NaN                1  \n",
       "1                NaN              NaN             NaN                1  \n",
       "2                NaN              NaN             NaN                1  \n",
       "3                NaN              NaN             NaN                1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'Text_PP__lemmatizer': [True, False],\n",
    "    'CVector__CountVector': [True, False]\n",
    "    # 'Resample__state': [0, 1, 2, 3],\n",
    "    # 'Resample__clusters': [5],\n",
    "    # 'Resample__k_neighbors': [50]\n",
    "}\n",
    "\n",
    "Vector = Vectorizer(CountVector=True, max_features=None)\n",
    "\n",
    "test_pipeline = Pipeline([\n",
    "    ('Text_PP', tokens),\n",
    "    ('CVector', Vector),\n",
    "    ('LR', lr)\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(test_pipeline, param_grid=param_grid, cv=2)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "results_df = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Shape: (21521,)\n",
      "CVector shape: (21521, 8000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster size: (672359, 13291), clusters = 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "      <th>freqs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categories</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3938</td>\n",
       "      <td>0.296291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3550</td>\n",
       "      <td>0.267098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2473</td>\n",
       "      <td>0.186066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1920</td>\n",
       "      <td>0.144459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1410</td>\n",
       "      <td>0.106087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            counts     freqs\n",
       "categories                  \n",
       "1             3938  0.296291\n",
       "2             3550  0.267098\n",
       "3             2473  0.186066\n",
       "4             1920  0.144459\n",
       "5             1410  0.106087"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample test\n",
    "\n",
    "tokens = Tokenizer(lemmatizer=True)\n",
    "\n",
    "Vector = Vectorizer(CountVector=True, max_features=8000)\n",
    "\n",
    "cutter = CutTransformer()\n",
    "\n",
    "sample=Sampler(state=3)\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "X_tokens = tokens.transform(X_train)\n",
    "X_vector = Vector.transform(X_tokens)\n",
    "X_sampled, y_sampled = sample.fit(X_vector, y_train)\n",
    "lr.fit(X_sampled, y_sampled)\n",
    "y_pred = lr.predict(X_sampled)\n",
    "y_cut = cutter.transform(y_pred)\n",
    "\n",
    "y_cut.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensional Reduction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso_reduction(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, alpha=1, max_iter=1000):\n",
    "            self.alpha = alpha\n",
    "            self.max_iter = max_iter\n",
    "        \n",
    "        def fit(self, X, y):\n",
    "            self.lasso = Lasso(alpha=self.alpha, max_iter=self.max_iter)\n",
    "            self.lasso.fit(X, y)\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            sfm = SelectFromModel(self.lasso, prefit=True)\n",
    "            reduced_X = sfm.transform(X)\n",
    "            return reduced_X\n",
    "\n",
    "class RandomForestReduction(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_features=None, n_estimators=100, max_depth=None):\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.rf = RandomForestRegressor(max_features=self.max_features, n_estimators=self.n_estimators, max_depth=self.max_depth)\n",
    "        self.rf.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        sfm = SelectFromModel(self.rf, prefit=True)\n",
    "        reduced_X = sfm.transform(X)\n",
    "        return reduced_X\n",
    "\n",
    "class XGBoostReduction(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_estimators=100, max_depth=None, learning_rate=0.1, threshold=5):\n",
    "        self.n_estimators= n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.boost_red = xgb.XGBRegressor(n_estimators=self.n_estimators, max_depth=self.max_depth, learning_rate=self.learning_rate)\n",
    "        self.boost_red.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        sfm = SelectFromModel(self.boost_red, threshold=self.threshold)\n",
    "        reduced_X = sfm.transform(X)\n",
    "        return reduced_X\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "dim_red_pipe = Pipeline([\n",
    "    ('lasso', lasso_reduction()),\n",
    "    ('RF', RandomForestReduction()),\n",
    "    ('XGBoost', XGBoostReduction()),\n",
    "    ('PCA', pca)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Shape: (21521,)\n",
      "CVector shape: (21521, 8000)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "np.matrix is not supported. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Documents\\GitHub\\amazon_review\\AdditionalRegressionAnalysis.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X53sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m X_tokens \u001b[39m=\u001b[39m tokens\u001b[39m.\u001b[39mtransform(X_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X53sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m X_vector \u001b[39m=\u001b[39m Vector\u001b[39m.\u001b[39mtransform(X_tokens)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X53sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m X_sampled, y_sampled \u001b[39m=\u001b[39m sample\u001b[39m.\u001b[39;49mfit(X_vector\u001b[39m.\u001b[39;49mtodense(), y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X53sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m lasso\u001b[39m.\u001b[39mfit(X_sampled, y_sampled)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X53sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m X_reduced \u001b[39m=\u001b[39m lasso\u001b[39m.\u001b[39mtransform(X_sampled)\n",
      "\u001b[1;32mc:\\Documents\\GitHub\\amazon_review\\AdditionalRegressionAnalysis.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X53sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X53sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     oversampler \u001b[39m=\u001b[39m RandomOverSampler(sampling_strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminority\u001b[39m\u001b[39m'\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m69\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X53sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     X_sampled, y_sampled \u001b[39m=\u001b[39m oversampler\u001b[39m.\u001b[39;49mfit_resample(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X53sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrOs size: \u001b[39m\u001b[39m{\u001b[39;00mX_sampled\u001b[39m.\u001b[39msize,\u001b[39m \u001b[39my_sampled\u001b[39m.\u001b[39msize\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X53sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m X_sampled, y_sampled\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_resample(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\base.py:106\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    104\u001b[0m check_classification_targets(y)\n\u001b[0;32m    105\u001b[0m arrays_transformer \u001b[39m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m--> 106\u001b[0m X, y, binarize_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X_y(X, y)\n\u001b[0;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy_ \u001b[39m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy, y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampling_type\n\u001b[0;32m    110\u001b[0m )\n\u001b[0;32m    112\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_resample(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\over_sampling\\_random_over_sampler.py:158\u001b[0m, in \u001b[0;36mRandomOverSampler._check_X_y\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_X_y\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[0;32m    157\u001b[0m     y, binarize_y \u001b[39m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 158\u001b[0m     X \u001b[39m=\u001b[39m _check_X(X)\n\u001b[0;32m    159\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_names(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\utils\\_validation.py:624\u001b[0m, in \u001b[0;36m_check_X\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m _is_pandas_df(X):\n\u001b[0;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m X\n\u001b[1;32m--> 624\u001b[0m \u001b[39mreturn\u001b[39;00m check_array(\n\u001b[0;32m    625\u001b[0m     X, dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m], force_all_finite\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    626\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:751\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Input validation on an array, list, sparse matrix or similar.\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \n\u001b[0;32m    662\u001b[0m \u001b[39mBy default, the input is checked to be a non-empty 2D array containing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[39m    The converted and validated array.\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(array, np\u001b[39m.\u001b[39mmatrix):\n\u001b[1;32m--> 751\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    752\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnp.matrix is not supported. Please convert to a numpy array with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    753\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnp.asarray. For more information see: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    754\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://numpy.org/doc/stable/reference/generated/numpy.matrix.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    755\u001b[0m     )\n\u001b[0;32m    757\u001b[0m xp, is_array_api_compliant \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    759\u001b[0m \u001b[39m# store reference to original array to check if copy is needed when\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[39m# function returns\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: np.matrix is not supported. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html"
     ]
    }
   ],
   "source": [
    "# Test Dimensional Reduction\n",
    "\n",
    "tokens = Tokenizer(lemmatizer=True)\n",
    "\n",
    "Vector = Vectorizer(CountVector=True, max_features=8000)\n",
    "\n",
    "cutter = CutTransformer()\n",
    "\n",
    "sample=Sampler(state=0)\n",
    "\n",
    "lasso = lasso_reduction()\n",
    "rf = RandomForestReduction()\n",
    "xbg = XGBoostReduction()\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "X_tokens = tokens.transform(X_train)\n",
    "X_vector = Vector.transform(X_tokens)\n",
    "X_sampled, y_sampled = sample.fit(X_vector.todense(), y_train)\n",
    "lasso.fit(X_sampled, y_sampled)\n",
    "X_reduced = lasso.transform(X_sampled)\n",
    "\n",
    "print(X_reduced.size)\n",
    "\n",
    "\n",
    "\n",
    "# test_pipeline = Pipeline([\n",
    "#     ('dim_red', dim_red_pipe),\n",
    "#     ('LR', transformed_lr_regressor)\n",
    "# ])\n",
    "\n",
    "# param_grid = {\n",
    "#     'dim_red__lasso__alpha': [0.5],\n",
    "#     'dim_red__lasso__max_iter': [5000],\n",
    "#     'dim_red__RF__max_features': ['sqrt'],\n",
    "#     'dim_red__RF__n_estimators': [250],\n",
    "#     'dim_red__RF__max_depth': [10],\n",
    "#     'dim_red__XGBoost__learning_rate': [0.1],\n",
    "#     'dim_red__XGBoost__n_estimators': [250],\n",
    "#     'dim_red__XGBoost__max_depth': [10],\n",
    "#     'dim_red__XGBoost__threshold': [25]\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(test_pipeline, param_grid=param_grid, cv=2)\n",
    "\n",
    "# grid.fit(X_sampled, y_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.664133624257016"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, regressor=LinearRegression(), transformer=None):\n",
    "        self.regressor = regressor\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.regressor.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        y_pred = self.regressor.predict(X)\n",
    "        y_pred_cut = self.transformer.transform(y_pred)\n",
    "        return y_pred_cut\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.regressor.predict(X)\n",
    "        y_pred_cut = self.transformer.transform(y_pred)\n",
    "        return y_pred_cut\n",
    "    \n",
    "    def score(self, X, y, custom_scoring='accuracy'):\n",
    "        y_pred = self.regressor.predict(X)\n",
    "        y_pred_cut = self.transformer.transform(y_pred)\n",
    "        if custom_scoring == 'accuracy':\n",
    "            return accuracy_score(y, y_pred_cut)\n",
    "        elif custom_scoring == 'precision':\n",
    "            return precision_score(y, y_pred_cut)\n",
    "        elif custom_scoring == 'recall':\n",
    "            return recall_score(y, y_pred_cut)\n",
    "        elif custom_scoring == 'f1_score':\n",
    "            return f1_score(y, y_pred_cut)\n",
    "        elif custom_scoring == 'r2_score':\n",
    "            return r2_score(y, y_pred_cut)\n",
    "        elif custom_scoring == 'mse':\n",
    "            return mean_squared_error(y, y_pred_cut)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid custom scoring metric\")\n",
    "        \n",
    "\n",
    "custom_reg = CustomRegressor(regressor=lr, transformer=cutter)\n",
    "\n",
    "custom_reg.fit(X_sampled, y_sampled)\n",
    "\n",
    "custom_reg.score(X_sampled, y_sampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 2 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 423, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 377, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 959, in _fit_transform_one\n    res = transformer.fit(X, y, **fit_params).transform(X)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Scratticus\\AppData\\Local\\Temp\\ipykernel_30396\\3126852135.py\", line 12, in transform\n    y_pred_cut = self.transformer.transform(y_pred)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Scratticus\\AppData\\Local\\Temp\\ipykernel_30396\\109930978.py\", line 6, in transform\n    X_cut = pd.cut(x=X, bins=[X.min(),1.5,2.5,3.5,4.5,X.max()], labels=[1,2,3,4,5], include_lowest=True)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\tile.py\", line 291, in cut\n    raise ValueError(\"bins must increase monotonically.\")\nValueError: bins must increase monotonically.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Documents\\GitHub\\amazon_review\\AdditionalRegressionAnalysis.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X54sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m test_pipeline \u001b[39m=\u001b[39m Pipeline([\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X54sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mLasso\u001b[39m\u001b[39m'\u001b[39m, custom_reg),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X54sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mRidge\u001b[39m\u001b[39m'\u001b[39m, custom_reg),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X54sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mElasticNet\u001b[39m\u001b[39m'\u001b[39m, custom_reg),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X54sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mGBR\u001b[39m\u001b[39m'\u001b[39m, custom_reg)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X54sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X54sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(test_pipeline, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X54sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m grid\u001b[39m.\u001b[39;49mfit(X_sampled, y_sampled)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X54sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m results_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(grid\u001b[39m.\u001b[39mcv_results_)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[0;32m    869\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    870\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    872\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[0;32m    873\u001b[0m     )\n\u001b[1;32m--> 875\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[0;32m    877\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:414\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    408\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    409\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m     )\n\u001b[1;32m--> 414\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    416\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    418\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 2 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 423, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 377, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py\", line 959, in _fit_transform_one\n    res = transformer.fit(X, y, **fit_params).transform(X)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Scratticus\\AppData\\Local\\Temp\\ipykernel_30396\\3126852135.py\", line 12, in transform\n    y_pred_cut = self.transformer.transform(y_pred)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 157, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Scratticus\\AppData\\Local\\Temp\\ipykernel_30396\\109930978.py\", line 6, in transform\n    X_cut = pd.cut(x=X, bins=[X.min(),1.5,2.5,3.5,4.5,X.max()], labels=[1,2,3,4,5], include_lowest=True)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\tile.py\", line 291, in cut\n    raise ValueError(\"bins must increase monotonically.\")\nValueError: bins must increase monotonically.\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'Lasso__regressor': [Lasso()],\n",
    "    'Lasso__regressor__alpha': [0.1],\n",
    "    'Ridge__regressor': [Ridge()],\n",
    "    'Ridge__regressor__alpha': [0.1],\n",
    "    'ElasticNet__regressor': [ElasticNet()],\n",
    "    'ElasticNet__regressor__alpha': [0.1],\n",
    "    'ElasticNet__regressor__l1_ratio': [0.7],\n",
    "    'GBR__regressor': [GradientBoostingRegressor()],\n",
    "    'GBR__regressor__learning_rate': [0.1],\n",
    "    'GBR__regressor__n_estimators': [250],\n",
    "    'GBR__regressor__max_depth': [10]\n",
    "}\n",
    "\n",
    "test_pipeline = Pipeline([\n",
    "    ('Lasso', custom_reg),\n",
    "    ('Ridge', custom_reg),\n",
    "    ('ElasticNet', custom_reg),\n",
    "    ('GBR', custom_reg)\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(test_pipeline, param_grid=param_grid, cv=2)\n",
    "\n",
    "grid.fit(X_sampled, y_sampled)\n",
    "\n",
    "results_df = pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'text_preprocessing__lemmatized__enabled': [True, False],\n",
    "    'resampling__SMOTE__k_neighbors': [2, 5, 10, 15, 25, 50, 100, 250, 500, 1000],\n",
    "    'resampling__SMOTE__kind': ['deprecated', 'regular', 'svm'],\n",
    "    'resampling__RandomUnderSampled__n_neighbors': [2, 5, 10, 15, 25, 50, 100, 250, 500, 1000],\n",
    "    'resampling__CC__clusters': [2, 5, 10, 15, 25, 50, 100, 250, 500, 1000],\n",
    "    'dim_red__lasso__alpha': np.logspace(-4, 0, 8, endpoint=True, base=10),\n",
    "    'dim_red__lasso__max_iter': [1000, 5000, 10000],\n",
    "    'dim_red__RF__max_features': ['auto', 'log2'],\n",
    "    'dim_red__RF__n_estimators': [50, 100, 250, 500, 1000, 2000, 4000],\n",
    "    'dim_red__RF__max_depth': [10, 20, 30, None],\n",
    "    'dim_red__XGBoost__max_learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'dim_red__XGBoost__n_estimators': [50, 100, 250, 500, 1000, 2000, 4000],\n",
    "    'dim_red__XGBoost__max_depth': [10, 20, 30, None],\n",
    "    'dim_red__XGBoost__threshold': [5, 10, 25, 50, 100, 250, 500, 1000, 1500, 2000, 2500, 5000],\n",
    "    'regression__Lasso__alpha': np.logspace(-4, 0, 8, endpoint=True, base=10),\n",
    "    'regression__Ridge__alpha': np.logspace(-4, 4, 9, endpoint=True, base=10),\n",
    "    'regression__ElasticNet__alpha': np.logspace(-4, 4, 9, endpoint=True, base=10),\n",
    "    'regression__ElasticNet__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'regression__GBR__max_learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'regression__GBR__n_estimators': [50, 100, 250, 500, 1000, 2000, 4000],\n",
    "    'regression__GBR__max_depth': [10, 20, 30, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = ([\n",
    "    ('text_preprocessing', text_pp_pipe),\n",
    "    ('Vectorizing', vector_pipe),\n",
    "    ('resampling', resample_pipe),\n",
    "    ('dim_red', dim_red_pipe),\n",
    "    ('regression', regression_pipe),\n",
    "    ('cut', cut)\n",
    "])\n",
    "\n",
    "regression_grid = GridSearchCV(full_pipeline, param_grid=param_grid, scoring=scoring, cv=5)\n",
    "regression_grid.fit(X_train, y_train)\n",
    "\n",
    "results_df = pd.DataFrame(regression_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sampler(X,y):\n",
    "    \"\"\"\n",
    "    This function resamples target (y) and (X) using the RandomOverSampler from the imblearn\n",
    "    oversampler library. The sampler uses the minority sampling strategy to suit the amazon\n",
    "    review data. There are no further arguments to adjust.\n",
    "    \n",
    "    Args:\n",
    "    Feature data, target data: These are expected to be tokenized, Vectorized array data.\n",
    "    Inputs should be training data only to avoid statistical leakage\n",
    "    \n",
    "    Returns:\n",
    "    resampled feature data, resampled target data. As tokenized vectorized arrays.\n",
    "\n",
    "    Example:\n",
    "    over_sampler(X_train, y_train)\n",
    "\n",
    "    returns:\n",
    "    X_train_oversampled, y_train_oversampled\n",
    "    \"\"\"\n",
    "    sampler = RandomOverSampler(sampling_strategy='minority', n_jobs=-1, random_state=69)\n",
    "    X_oversampled, y_oversampled = sampler.fit_resample(X,y)\n",
    "    return X_oversampled, y_oversampled\n",
    "\n",
    "def smote_sampler(X, y, k, kind):\n",
    "    \"\"\"\n",
    "    This function resamples target (y) and (X) using the SMOTE resampler from the imblearn\n",
    "    oversampler library. The sampler uses the minority sampling strategy to suit the amazon\n",
    "    review data. The model can be further optimized by cycling kind methods with a parameter\n",
    "    grid\n",
    "    \n",
    "    Args:\n",
    "    Feature data, target data: These are expected to be tokenized, Vectorized array data.\n",
    "    Inputs should be training data only to avoid statistical leakage\n",
    "    \n",
    "    Returns:\n",
    "    resampled feature data, resampled target data. As tokenized vectorized arrays.\n",
    "\n",
    "    Example:\n",
    "    smote_sampler(X_train, y_train, k=1000, kind='regular')\n",
    "\n",
    "    returns:\n",
    "    X_train_oversampled, y_train_oversampled\n",
    "    \"\"\"\n",
    "    sampler = SMOTE(sampling_strategy='minority', n_jobs=-1, random_state=69, k_neighbors=k, kind=kind)\n",
    "    X_oversampled, y_oversampled = sampler.fit_resample(X,y)\n",
    "    return X_oversampled, y_oversampled\n",
    "\n",
    "def under_sampler(X, y, k):\n",
    "    \"\"\"\n",
    "    This function resamples target (y) and (X) using the RandomUnderSampler from the imblearn\n",
    "    undersampler library. The sampler uses the majority sampling strategy to suit the amazon\n",
    "    review data. The model can be further optimized by cycling the n_neighbours argument\n",
    "    values with a parameter grid\n",
    "    \n",
    "    Args:\n",
    "    Feature data, target data: These are expected to be tokenized, Vectorized array data.\n",
    "    Inputs should be training data only to avoid statistical leakage\n",
    "    \n",
    "    Returns:\n",
    "    resampled feature data, resampled target data. As tokenized vectorized arrays.\n",
    "\n",
    "    Example:\n",
    "    under_sampler(X_train, y_train, k=1000)\n",
    "\n",
    "    returns:\n",
    "    X_train_undersampled, y_train_undersampled\n",
    "    \"\"\"\n",
    "    sampler = RandomUnderSampler(sampling_strategy='majority', n_jobs=-1, random_state=69, n_neighbors=k)\n",
    "    X_undersampled, y_undersampled = sampler.fit_resample(X,y)\n",
    "    return X_undersampled, y_undersampled\n",
    "\n",
    "def cluster_sampler(X, y, k):\n",
    "    \"\"\"\n",
    "    This function resamples target (y) and (X) using the ClusterCentroids resampler from the \n",
    "    imblearn undersampler library. To maintain simplicity, this function uses the Kmeans \n",
    "    cluster method only, the n_clusters can be tuned to fit the data better and further\n",
    "    cluster methods can be explored if the resampler provides good scores.\n",
    "    \n",
    "    Args:\n",
    "    Feature data, target data: These are expected to be tokenized, Vectorized array data.\n",
    "    Inputs should be training data only to avoid statistical leakage\n",
    "    \n",
    "    Returns:\n",
    "    resampled feature data, resampled target data. As tokenized vectorized arrays.\n",
    "\n",
    "    Example:\n",
    "    cluster_sampler(X_train, y_train, k=5)\n",
    "\n",
    "    returns:\n",
    "    X_train_undersampled, y_train_undersampled\n",
    "    \"\"\"\n",
    "    estimator = KMeans(n_clusters=k, random_state=69)\n",
    "    sampler = ClusterCentroids(sampling_strategy='majority', n_jobs=-1, random_state=69, estimator=estimator)\n",
    "    X_undersampled, y_undersampled = sampler.fit_resample(X,y)\n",
    "    return X_undersampled, y_undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
