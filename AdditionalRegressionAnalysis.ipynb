{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Regression Analyses\n",
    "## Summary and Initial Investigation Recap\n",
    "The Initial Regression investigation showed that the best results were found with the non-sampled stemmatized reviews which were vectorized with the Term Frequency-Inverse Document Frequency (TFIDF) method. \n",
    "\n",
    "Regression was measured using the LinearRegression model only and the best prediction of the test sample provided by this data included the following scores:\n",
    "* Coefficient: 0.755\n",
    "* R Squared Value: 0.612\n",
    "* P Value: 0 to at least 5dp\n",
    "* Precision: 0.39\n",
    "* F1 Score: 0.34\n",
    "* Geometric Mean: 0.52\n",
    "\n",
    "These scores are not good enough to accurately provide a sentiment analysis of the reviews, but the P Value is very promising.\n",
    "\n",
    "The initial Analysis also revealed that the text data could be adjusted to fit the overall star rating from 1-5, however the final results must be cut into values of 1 to 5 integers otherwise the errors contained in the float decimal skew the regression error metrics like the geometric mean. It was also observed that the predicted values were not contained within the 1 - 5 bounds and a small amount of values drifted beyond Â±10.\n",
    "\n",
    "The data was unbalanced, and due to the quantity of reviews available only under sampling resampling was attempted on the training data, however the resampled data had a negative impact on all scores.\n",
    "\n",
    "## Hypothesis\n",
    "The Initial Regression Analysis P Value indicates that there is indeed a correlation between the reviews and the review rating scores from 1-5, since the linear regression model did not provide satisfactory scores the correlation may not be linear.\n",
    "\n",
    "The resampling was not sufficient in the Initial exploration, however we can clearly see that the datset is imbalanced with 71.9% of the values falling in 1 and 5. Due to the amount of data only under sampling was implemented, however by reducing the size of the training set other sampling methods could be implemented. Of course different sampling methods may affect the data differently, so it may be necessary to lemmatize and stemmatize the original texts for different methods. Similarly different models may provide better scores with the vectorization methods.\n",
    "\n",
    "By implementing pipelines and GridSearchCV, several regression models can be tested quickly and return the best scores and parameters for each model can be returned. In this notebook the data will be lemmatized, stemmatized, CountVectorized and TFIDF Vectorized as before. In addition the training set will be halved and RandomOverSampling, Synthetic Minority Over-sampling (SMOTE) and CentroidCluster resampling techniques will be implemented alongside RandomUndersampling. The raw data alongside the resampled data will be dimensionally reduced using Lasso, Random Forest, XGBoost and Principle Component Analysis (PCA) Methods. Finally Regression analyses will be performed by fitting and predicting data using Lasso, Ridge, ElasticNet and GradientBoostingRegressor models.\n",
    "\n",
    "It may be that the data is better suited to Classification than Regression, and so whilst this document is prepared to analyse Regression a second notebook is being prepared to model the data with classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextMiningProcesses import column_lemmatizer, column_stemmatizer, count_vectorize_data, tfidf_vectorize_data\n",
    "\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.feature_selection import RFE, SelectKBest, SelectFromModel\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, LinearRegression\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train - Test Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of Training data records: 43043\n",
      "Original number of Test data records:18447\n",
      "Training data will be shortened to enable resampling within a reasonable timeframe\n",
      "New number of Training data records 21521\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# View length of data\n",
    "print(f'Original number of Training data records: {len(train)}')\n",
    "print(f'Original number of Test data records:{len(test)}')\n",
    "\n",
    "# half the amount of training data for sampling\n",
    "half_train = train.iloc[:len(train) // 2, :]\n",
    "\n",
    "# View length of data\n",
    "print(f'Training data will be shortened to enable resampling within a reasonable timeframe')\n",
    "print(f'New number of Training data records {len(half_train)}')\n",
    "\n",
    "# Create X_train, X_test, y_train, y_test variables\n",
    "X_train = half_train['reviewText']\n",
    "y_train = half_train['overall']\n",
    "X_test = test['reviewText']\n",
    "y_test = test['overall']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, enabled=True):\n",
    "        self.enabled = enabled\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.enabled:\n",
    "            X_lem = column_lemmatizer(X)\n",
    "            return X_lem\n",
    "        else:\n",
    "            X_stem = column_stemmatizer(X)\n",
    "            return X_stem\n",
    "    \n",
    "tokens = Tokenizer(enabled=True)\n",
    "\n",
    "text_pp_pipe = Pipeline([\n",
    "    ('tokenized', tokens)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought cooker order able make chinese hot pot home instruction set easy follow even come cooking pot box included magnet make easy test pan pot see work cooker purchasing setting simple easy use cord nice enough length extend outlet kitchen table happy purchase wait find us \n"
     ]
    }
   ],
   "source": [
    "# Testing text functions\n",
    "\n",
    "tokens = Tokenizer(enabled=False)\n",
    "\n",
    "processed_text = text_pp_pipe.transform(X_train)\n",
    "\n",
    "print(processed_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, enabled=True):\n",
    "        self.enabled = enabled\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.enabled:\n",
    "            CV_X = count_vectorize_data(X)\n",
    "            return CV_X\n",
    "        else:\n",
    "            TFIDF_X = tfidf_vectorize_data(X)\n",
    "            return TFIDF_X\n",
    "        \n",
    "Vector = Vectorizer(enabled=True)\n",
    "\n",
    "vector_pipe = Pipeline([\n",
    "    ('Vector', Vector)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "variable = None\n",
    "\n",
    "if variable:\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Documents\\GitHub\\amazon_review\\AdditionalRegressionAnalysis.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X45sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m test_pipeline \u001b[39m=\u001b[39m Pipeline([\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X45sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mtext_preprocessing\u001b[39m\u001b[39m'\u001b[39m, text_pp_pipe),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X45sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mVectorizing\u001b[39m\u001b[39m'\u001b[39m, vector_pipe),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X45sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mLR\u001b[39m\u001b[39m'\u001b[39m, lr)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X45sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X45sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(test_pipeline, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X45sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m grid\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X45sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m results_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(grid\u001b[39m.\u001b[39mcv_results_)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X45sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m results_df\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    728\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    731\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:423\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \n\u001b[0;32m    399\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    422\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 423\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[0;32m    424\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:377\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    375\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    376\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    378\u001b[0m     cloned_transformer,\n\u001b[0;32m    379\u001b[0m     X,\n\u001b[0;32m    380\u001b[0m     y,\n\u001b[0;32m    381\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    382\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    383\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[0;32m    384\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[0;32m    385\u001b[0m )\n\u001b[0;32m    386\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:957\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 957\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    958\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    959\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:479\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    477\u001b[0m fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(last_step, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 479\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39;49mfit_transform(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[0;32m    480\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    481\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\u001b[39m.\u001b[39mtransform(Xt)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:919\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    916\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    917\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    918\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 919\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "\u001b[1;32mc:\\Documents\\GitHub\\amazon_review\\AdditionalRegressionAnalysis.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X45sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menabled:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X45sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         X_lem \u001b[39m=\u001b[39m column_lemmatizer(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X45sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m X_lem\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Documents/GitHub/amazon_review/AdditionalRegressionAnalysis.ipynb#X45sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Documents\\GitHub\\amazon_review\\TextMiningProcesses.py:52\u001b[0m, in \u001b[0;36mcolumn_lemmatizer\u001b[1;34m(text_series)\u001b[0m\n\u001b[0;32m     50\u001b[0m lemmed \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m stopped:\n\u001b[1;32m---> 52\u001b[0m     lemmed_word \u001b[39m=\u001b[39m lemmatizer\u001b[39m.\u001b[39;49mlemmatize(word)\n\u001b[0;32m     53\u001b[0m     lemmed \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m lemmed_word \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     55\u001b[0m lemmed\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m, pos: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49m_morphy(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m(lemmas, key\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m) \u001b[39mif\u001b[39;00m lemmas \u001b[39melse\u001b[39;00m word\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2100\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   2097\u001b[0m         \u001b[39mreturn\u001b[39;00m filter_forms([form] \u001b[39m+\u001b[39m exceptions[form])\n\u001b[0;32m   2099\u001b[0m \u001b[39m# 1. Apply rules once to the input to get y1, y2, y3, etc.\u001b[39;00m\n\u001b[1;32m-> 2100\u001b[0m forms \u001b[39m=\u001b[39m apply_rules([form])\n\u001b[0;32m   2102\u001b[0m \u001b[39m# 2. Return all that are in the database (and check the original too)\u001b[39;00m\n\u001b[0;32m   2103\u001b[0m results \u001b[39m=\u001b[39m filter_forms([form] \u001b[39m+\u001b[39m forms)\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2076\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy.<locals>.apply_rules\u001b[1;34m(forms)\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_rules\u001b[39m(forms):\n\u001b[1;32m-> 2076\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m   2077\u001b[0m         form[: \u001b[39m-\u001b[39;49m\u001b[39mlen\u001b[39;49m(old)] \u001b[39m+\u001b[39;49m new\n\u001b[0;32m   2078\u001b[0m         \u001b[39mfor\u001b[39;49;00m form \u001b[39min\u001b[39;49;00m forms\n\u001b[0;32m   2079\u001b[0m         \u001b[39mfor\u001b[39;49;00m old, new \u001b[39min\u001b[39;49;00m substitutions\n\u001b[0;32m   2080\u001b[0m         \u001b[39mif\u001b[39;49;00m form\u001b[39m.\u001b[39;49mendswith(old)\n\u001b[0;32m   2081\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Scratticus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2076\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_rules\u001b[39m(forms):\n\u001b[1;32m-> 2076\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m   2077\u001b[0m         form[: \u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(old)] \u001b[39m+\u001b[39m new\n\u001b[0;32m   2078\u001b[0m         \u001b[39mfor\u001b[39;00m form \u001b[39min\u001b[39;00m forms\n\u001b[0;32m   2079\u001b[0m         \u001b[39mfor\u001b[39;00m old, new \u001b[39min\u001b[39;00m substitutions\n\u001b[0;32m   2080\u001b[0m         \u001b[39mif\u001b[39;00m form\u001b[39m.\u001b[39mendswith(old)\n\u001b[0;32m   2081\u001b[0m     ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Testing Functions\n",
    "\n",
    "param_grid = {\n",
    "    'text_preprocessing__tokenized__enabled': [True, False],\n",
    "    'Vectorizing__Vector__enabled': [True, False]\n",
    "}\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "test_pipeline = Pipeline([\n",
    "    ('text_preprocessing', text_pp_pipe),\n",
    "    ('Vectorizing', vector_pipe),\n",
    "    ('LR', lr)\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(test_pipeline, param_grid=param_grid, cv=5)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "results_df = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cluster_sampler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, clusters):\n",
    "        self.estimator = KMeans(n_clusters=clusters, random_state=69)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        sampler = ClusterCentroids(sampling_strategy='majority', n_jobs=-1, random_state=69, estimator=self.estimator)\n",
    "        X_undersampled, y_undersampled = sampler.fit_resample(X_train, y_train)\n",
    "        return X_undersampled, y_undersampled\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "oversampler = RandomOverSampler(sampling_strategy='minority', n_jobs=-1, random_state=69)\n",
    "smotesampler = SMOTE(sampling_strategy='minority', n_jobs=-1, random_state=69)\n",
    "undersampler = RandomUnderSampler(sampling_strategy='majority', n_jobs=-1, random_state=69)\n",
    "\n",
    "resample_pipe = ([\n",
    "    ('RandomOverSampled', oversampler),\n",
    "    ('SMOTE', smotesampler),\n",
    "    ('RandomUnderSampled', undersampler),\n",
    "    ('CC', cluster_sampler)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensional Reduction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lasso_reduction(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, alpha=1, max_iter=1000):\n",
    "            self.alpha = alpha\n",
    "            self.max_iter = max_iter\n",
    "        \n",
    "        def fit(self, X, y):\n",
    "            self.lasso = Lasso(alpha=self.alpha, max_iter=self.max_iter)\n",
    "            self.lasso.fit(X, y)\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            sfm = SelectFromModel(self.lasso, prefit=True)\n",
    "            reduced_X = sfm.transform(X)\n",
    "            return reduced_X\n",
    "\n",
    "class RandomForestReduction(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_features=None, n_estimators=100, max_depth=None):\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.rf = RandomForestRegressor(max_features=self.max_features, n_estimators=self.n_estimators, max_depth=self.max_depth)\n",
    "        self.rf.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        sfm = SelectFromModel(self.rf, prefit=True)\n",
    "        reduced_X = sfm.transform(X)\n",
    "        return reduced_X\n",
    "\n",
    "class XGBoostReduction(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_estimators=100, max_depth=None, learning_rate=0.1, threshold=5):\n",
    "        self.n_estimators= n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.boost_red = xgb.XGBRegressor(n_estimators=self.n_estimators, max_depth=self.max_depth, learning_rate=self.learning_rate)\n",
    "        self.boost_red.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        sfm = SelectFromModel(self.boost_red, threshold=self.threshold)\n",
    "        reduced_X = sfm.transform(X)\n",
    "        return reduced_X\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "dim_red_pipe = Pipeline([\n",
    "    ('lasso', lasso_reduction()),\n",
    "    ('RF', RandomForestReduction()),\n",
    "    ('XGBoost', XGBoostReduction()),\n",
    "    ('PCA', pca)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_pipe = Pipeline([\n",
    "    ('LinearRegression', LinearRegression()),\n",
    "    ('Lasso', Lasso()),\n",
    "    ('Ridge', Ridge()),\n",
    "    ('ElasticNet', ElasticNet()),\n",
    "    ('GBR', GradientBoostingRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cutting and Scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cut():\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return pd.cut(x=X, bins=[X.min(),1.5,2.5,3.5,4.5,X.max()], labels=[1,2,3,4,5], include_lowest=True)\n",
    "    \n",
    "\n",
    "precision_scorer = make_scorer(precision_score, average='weighted')\n",
    "recall_scorer = make_scorer(recall_score, average='weighted')\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "    \n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': precision_scorer,\n",
    "    'recall': recall_scorer,\n",
    "    'f1_score': f1_scorer,\n",
    "    'r2_score': 'r2',\n",
    "    'mse': mse_scorer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'text_preprocessing__lemmatized__enabled': [True, False],\n",
    "    'resampling__SMOTE__k_neighbors': [2, 5, 10, 15, 25, 50, 100, 250, 500, 1000],\n",
    "    'resampling__SMOTE__kind': ['deprecated', 'regular', 'svm'],\n",
    "    'resampling__RandomUnderSampled__n_neighbors': [2, 5, 10, 15, 25, 50, 100, 250, 500, 1000],\n",
    "    'resampling__CC__clusters': [2, 5, 10, 15, 25, 50, 100, 250, 500, 1000],\n",
    "    'dim_red__lasso__alpha': np.logspace(-4, 0, 8, endpoint=True, base=10),\n",
    "    'dim_red__lasso__max_iter': [1000, 5000, 10000],\n",
    "    'dim_red__RF__max_features': ['auto', 'log2'],\n",
    "    'dim_red__RF__n_estimators': [50, 100, 250, 500, 1000, 2000, 4000],\n",
    "    'dim_red__RF__max_depth': [10, 20, 30, None],\n",
    "    'dim_red__XGBoost__max_learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'dim_red__XGBoost__n_estimators': [50, 100, 250, 500, 1000, 2000, 4000],\n",
    "    'dim_red__XGBoost__max_depth': [10, 20, 30, None],\n",
    "    'dim_red__XGBoost__threshold': [5, 10, 25, 50, 100, 250, 500, 1000, 1500, 2000, 2500, 5000],\n",
    "    'regression__Lasso__alpha': np.logspace(-4, 0, 8, endpoint=True, base=10),\n",
    "    'regression__Ridge__alpha': np.logspace(-4, 4, 9, endpoint=True, base=10),\n",
    "    'regression__ElasticNet__alpha': np.logspace(-4, 4, 9, endpoint=True, base=10),\n",
    "    'regression__ElasticNet__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'regression__GBR__max_learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'regression__GBR__n_estimators': [50, 100, 250, 500, 1000, 2000, 4000],\n",
    "    'regression__GBR__max_depth': [10, 20, 30, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = ([\n",
    "    ('text_preprocessing', text_pp_pipe),\n",
    "    ('Vectorizing', vector_pipe),\n",
    "    ('resampling', resample_pipe),\n",
    "    ('dim_red', dim_red_pipe),\n",
    "    ('regression', regression_pipe),\n",
    "    ('cut', cut)\n",
    "])\n",
    "\n",
    "regression_grid = GridSearchCV(full_pipeline, param_grid=param_grid, scoring=scoring, cv=5)\n",
    "regression_grid.fit(X_train, y_train)\n",
    "\n",
    "results_df = pd.DataFrame(regression_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sampler(X,y):\n",
    "    \"\"\"\n",
    "    This function resamples target (y) and (X) using the RandomOverSampler from the imblearn\n",
    "    oversampler library. The sampler uses the minority sampling strategy to suit the amazon\n",
    "    review data. There are no further arguments to adjust.\n",
    "    \n",
    "    Args:\n",
    "    Feature data, target data: These are expected to be tokenized, Vectorized array data.\n",
    "    Inputs should be training data only to avoid statistical leakage\n",
    "    \n",
    "    Returns:\n",
    "    resampled feature data, resampled target data. As tokenized vectorized arrays.\n",
    "\n",
    "    Example:\n",
    "    over_sampler(X_train, y_train)\n",
    "\n",
    "    returns:\n",
    "    X_train_oversampled, y_train_oversampled\n",
    "    \"\"\"\n",
    "    sampler = RandomOverSampler(sampling_strategy='minority', n_jobs=-1, random_state=69)\n",
    "    X_oversampled, y_oversampled = sampler.fit_resample(X,y)\n",
    "    return X_oversampled, y_oversampled\n",
    "\n",
    "def smote_sampler(X, y, k, kind):\n",
    "    \"\"\"\n",
    "    This function resamples target (y) and (X) using the SMOTE resampler from the imblearn\n",
    "    oversampler library. The sampler uses the minority sampling strategy to suit the amazon\n",
    "    review data. The model can be further optimized by cycling kind methods with a parameter\n",
    "    grid\n",
    "    \n",
    "    Args:\n",
    "    Feature data, target data: These are expected to be tokenized, Vectorized array data.\n",
    "    Inputs should be training data only to avoid statistical leakage\n",
    "    \n",
    "    Returns:\n",
    "    resampled feature data, resampled target data. As tokenized vectorized arrays.\n",
    "\n",
    "    Example:\n",
    "    smote_sampler(X_train, y_train, k=1000, kind='regular')\n",
    "\n",
    "    returns:\n",
    "    X_train_oversampled, y_train_oversampled\n",
    "    \"\"\"\n",
    "    sampler = SMOTE(sampling_strategy='minority', n_jobs=-1, random_state=69, k_neighbors=k, kind=kind)\n",
    "    X_oversampled, y_oversampled = sampler.fit_resample(X,y)\n",
    "    return X_oversampled, y_oversampled\n",
    "\n",
    "def under_sampler(X, y, k):\n",
    "    \"\"\"\n",
    "    This function resamples target (y) and (X) using the RandomUnderSampler from the imblearn\n",
    "    undersampler library. The sampler uses the majority sampling strategy to suit the amazon\n",
    "    review data. The model can be further optimized by cycling the n_neighbours argument\n",
    "    values with a parameter grid\n",
    "    \n",
    "    Args:\n",
    "    Feature data, target data: These are expected to be tokenized, Vectorized array data.\n",
    "    Inputs should be training data only to avoid statistical leakage\n",
    "    \n",
    "    Returns:\n",
    "    resampled feature data, resampled target data. As tokenized vectorized arrays.\n",
    "\n",
    "    Example:\n",
    "    under_sampler(X_train, y_train, k=1000)\n",
    "\n",
    "    returns:\n",
    "    X_train_undersampled, y_train_undersampled\n",
    "    \"\"\"\n",
    "    sampler = RandomUnderSampler(sampling_strategy='majority', n_jobs=-1, random_state=69, n_neighbors=k)\n",
    "    X_undersampled, y_undersampled = sampler.fit_resample(X,y)\n",
    "    return X_undersampled, y_undersampled\n",
    "\n",
    "def cluster_sampler(X, y, k):\n",
    "    \"\"\"\n",
    "    This function resamples target (y) and (X) using the ClusterCentroids resampler from the \n",
    "    imblearn undersampler library. To maintain simplicity, this function uses the Kmeans \n",
    "    cluster method only, the n_clusters can be tuned to fit the data better and further\n",
    "    cluster methods can be explored if the resampler provides good scores.\n",
    "    \n",
    "    Args:\n",
    "    Feature data, target data: These are expected to be tokenized, Vectorized array data.\n",
    "    Inputs should be training data only to avoid statistical leakage\n",
    "    \n",
    "    Returns:\n",
    "    resampled feature data, resampled target data. As tokenized vectorized arrays.\n",
    "\n",
    "    Example:\n",
    "    cluster_sampler(X_train, y_train, k=5)\n",
    "\n",
    "    returns:\n",
    "    X_train_undersampled, y_train_undersampled\n",
    "    \"\"\"\n",
    "    estimator = KMeans(n_clusters=k, random_state=69)\n",
    "    sampler = ClusterCentroids(sampling_strategy='majority', n_jobs=-1, random_state=69, estimator=estimator)\n",
    "    X_undersampled, y_undersampled = sampler.fit_resample(X,y)\n",
    "    return X_undersampled, y_undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
