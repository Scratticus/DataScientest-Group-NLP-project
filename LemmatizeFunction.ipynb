{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39ceb744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform lemmatization and remove stopwords\n",
    "def lemmatize_text(text_series):\n",
    "    \"\"\"\n",
    "    Perform lemmatization on the input text and remove common English stopwords.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "    str: The lemmatized text with stopwords removed.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from wordcloud import WordCloud\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.tokenize.regexp import RegexpTokenizer\n",
    "\n",
    "    # Download NLTK resources\n",
    "    if not nltk.corpus.stopwords.fileids():\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "        \n",
    "\n",
    "    # Initialize the lemmatizer and stopwords set\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenizer = RegexpTokenizer(r'[\\w]+')\n",
    "\n",
    "    lemmed_cells = []\n",
    "\n",
    "    for i in text_series:\n",
    "        \n",
    "        #tokenize series to lower case\n",
    "        tokens = tokenizer.tokenize(i.lower())\n",
    "\n",
    "        # remove stop words\n",
    "        stopped = []\n",
    "        for token in tokens:\n",
    "            if token not in stop_words:\n",
    "                stopped.append(token)\n",
    "\n",
    "        lemmed = ''\n",
    "        for word in stopped:\n",
    "            lemmed_word = lemmatizer.lemmatize(word)\n",
    "            lemmed += lemmed_word + ' '\n",
    "\n",
    "        lemmed.strip()\n",
    "\n",
    "        lemmed_cells.append(lemmed)\n",
    "\n",
    "    lemmed_series = pd.Series(lemmed_cells)\n",
    "\n",
    "    return lemmed_series\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e822bd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Scratticus\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb9a8f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    favourite thing harry potter hermione granger ...\n",
      "1                   love lord ring movie book justice \n",
      "2                    wish never seen looper load shit \n",
      "3                                      like good time \n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test1 = pd.Series({\n",
    "    0: \"My favourite thing about Harry Potter is hermione Granger 5 stars!\",\n",
    "    1: \"I love lord of the rings, but this movie didn't do the books justice\",\n",
    "    2: \"I wish I had never seen looper, what a load of shit\",\n",
    "    3: \"I like this, good times\"\n",
    "    })\n",
    "\n",
    "test1_return = lemmatize_text(test1)\n",
    "\n",
    "print(test1_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c42e1d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    favorite book android dream electric sheep \n",
      "1                          arrive 1 00pm 4 30pm \n",
      "2                             special character \n",
      "3                                   worry happy \n",
      "4                  example sentence punctuation \n",
      "5                                  12345 number \n",
      "6                                 beautiful day \n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "test2_data = ({\n",
    "    'test_text': [\n",
    "        \"My favorite book is 'Do android's dream of Electric Sheep'.\",\n",
    "        \"I'll arrive between 1.00pm and 4.30pm.\",\n",
    "        \"These are special characters #%^$@*&$#&^$\",\n",
    "        \"Don't worry, be happy!\",\n",
    "        \"This is just - an example, sentence with some; punctuation.\",\n",
    "        \"12345 is a number.\",\n",
    "        \"It's a beautiful day.\",\n",
    "        ]\n",
    "    })\n",
    "\n",
    "test2 = pd.DataFrame(test2_data)\n",
    "\n",
    "test2_return = lemmatize_text(test2['test_text'])\n",
    "\n",
    "print(test2_return)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
